import feedparser
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import asyncio
import os

class BBCNewsCrawler:
    def __init__(self):
        self.rss_feeds = {
            'world': 'http://feeds.bbci.co.uk/news/world/rss.xml',
            'technology': 'http://feeds.bbci.co.uk/news/technology/rss.xml',
            'business': 'http://feeds.bbci.co.uk/news/business/rss.xml',
            'science': 'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml',
            'entertainment': 'http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml',
            'health': 'http://feeds.bbci.co.uk/news/health/rss.xml'
        }
    
    async def get_today_news(self, category='world', limit=10):
        """ì˜¤ëŠ˜ BBC ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        if category not in self.rss_feeds:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
            return []
        
        try:
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(self.rss_feeds[category])
            today = datetime.now().date()
            
            today_news = []
            for entry in feed.entries[:limit]:
                # ë°œí–‰ì¼ í™•ì¸
                pub_date = datetime(*entry.published_parsed[:6])
                if pub_date.date() == today:
                    news_item = {
                        'title': entry.title,
                        'link': entry.link,
                        'summary': entry.summary,
                        'published': pub_date,
                        'category': category
                    }
                    today_news.append(news_item)
            
            print(f"âœ… BBC {category} ë‰´ìŠ¤ {len(today_news)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return today_news
            
        except Exception as e:
            print(f"âŒ BBC ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_all_categories_today(self, limit_per_category=5):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ì˜¤ëŠ˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        all_news = []
        
        for category in self.rss_feeds.keys():
            print(f"ğŸ“° {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘...")
            category_news = await self.get_today_news(category, limit_per_category)
            all_news.extend(category_news)
            await asyncio.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        return all_news
    
    async def get_article_content(self, url):
        """BBC ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # BBC ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('article')
            if article_body:
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                paragraphs = article_body.find_all('p')
                content = '\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                return content
            
            return None
            
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None

    async def save_to_file(self, news_list, category='all'):
        """ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥ (ë³¸ë¬¸ í¬í•¨)"""
        if not news_list:
            print("ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        today_str = datetime.now().strftime('%Y-%m-%d')
        os.makedirs('bbc_news', exist_ok=True)
        filename = f"bbc_news/bbc_{category}_{today_str}.md"
        
        # ê¸°ì‚¬ë³„ ë³¸ë¬¸ ë¹„ë™ê¸° ìˆ˜ì§‘
        print("ğŸ“ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
        tasks = [self.get_article_content(news['link']) for news in news_list]
        contents = await asyncio.gather(*tasks)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# BBC {category.capitalize()} News - {today_str}\n\n")
            for idx, news in enumerate(news_list):
                f.write(f"## {news['title']}\n")
                f.write(f"- ë§í¬: [{news['link']}]({news['link']})\n")
                f.write(f"- ë°œí–‰ì¼: {news['published']}\n")
                f.write(f"- ìš”ì•½: {news['summary']}\n")
                content = contents[idx]
                if content:
                    f.write(f"\n### ë³¸ë¬¸\n{content}\n")
                else:
                    f.write("\n### ë³¸ë¬¸\n(ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)\n")
                f.write("\n---\n\n")
        print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    crawler = BBCNewsCrawler()
    
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì˜¤ëŠ˜ ë‰´ìŠ¤
    world_news = await crawler.get_today_news('world', 5)
    for news in world_news:
        print(f"ğŸ“° {news['title']}")
        print(f"ğŸ”— {news['link']}")
        print(f"ğŸ“… {news['published']}")
        print("-" * 50)
    # íŒŒì¼ ì €ì¥ (ë³¸ë¬¸ í¬í•¨)
    await crawler.save_to_file(world_news, category='world')
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤
    all_news = await crawler.get_all_categories_today(3)
    print(f"ğŸ“Š ì´ {len(all_news)}ê°œì˜ ì˜¤ëŠ˜ ë‰´ìŠ¤ ìˆ˜ì§‘")
    # íŒŒì¼ ì €ì¥ (ë³¸ë¬¸ í¬í•¨)
    await crawler.save_to_file(all_news, category='all')

if __name__ == "__main__":
    asyncio.run(main()) 