import requests
import json
import time
from datetime import datetime
from urllib.parse import quote
from bs4 import BeautifulSoup

class FinalThreadsCrawler:
    def __init__(self):
        self.base_url = "https://www.threads.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
        })
    
    def search_threads(self, keyword, max_posts=5):
        """Threadsì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ (ì˜¬ë°”ë¥¸ URL êµ¬ì¡° ì‚¬ìš©)"""
        try:
            # ì˜¬ë°”ë¥¸ ê²€ìƒ‰ URL êµ¬ì¡°
            search_url = f"{self.base_url}/search?q={quote(keyword)}&serp_type=default&hl=ko"
            
            print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¤‘...")
            print(f"ğŸ“± URL: {search_url}")
            
            # ìš”ì²­ ë³´ë‚´ê¸°
            response = self.session.get(search_url, timeout=15)
            
            print(f"ğŸ“Š HTTP ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            if response.status_code == 200:
                print("âœ… í˜ì´ì§€ ë¡œë”© ì„±ê³µ!")
                return self.parse_threads_page(response.text, keyword, max_posts)
            else:
                print(f"âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                print(f"ğŸ“„ ì‘ë‹µ ë‚´ìš© ì¼ë¶€: {response.text[:500]}...")
                return []
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return []
    
    def parse_threads_page(self, html_content, keyword, max_posts):
        """HTMLì—ì„œ Threads í¬ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        results = []
        
        try:
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            print(f"ğŸ“„ HTML í¬ê¸°: {len(html_content)} ë¬¸ì")
            
            # í˜ì´ì§€ ì œëª© í™•ì¸
            title = soup.find('title')
            if title:
                print(f"ğŸ“‹ í˜ì´ì§€ ì œëª©: {title.text}")
            
            # ë©”íƒ€ íƒœê·¸ í™•ì¸
            meta_description = soup.find('meta', attrs={'name': 'description'})
            if meta_description:
                print(f"ğŸ“ ë©”íƒ€ ì„¤ëª…: {meta_description.get('content', '')[:100]}...")
            
            # Threads íŠ¹ì • êµ¬ì¡° ì°¾ê¸°
            posts = self.find_threads_posts(soup, keyword)
            
            if posts:
                print(f"ğŸ“Š Threads í¬ìŠ¤íŠ¸ {len(posts)}ê°œ ë°œê²¬")
                
                # ê²°ê³¼ ìƒì„±
                for i, post_data in enumerate(posts[:max_posts], 1):
                    post_data['index'] = i
                    post_data['keyword'] = keyword
                    post_data['crawled_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    results.append(post_data)
                    self.print_post(post_data, i)
            else:
                print("ğŸ” Threads í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
                results = self.fallback_text_search(soup, keyword, max_posts)
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def find_threads_posts(self, soup, keyword):
        """Threads í¬ìŠ¤íŠ¸ êµ¬ì¡° ì°¾ê¸°"""
        posts = []
        
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ Threads í¬ìŠ¤íŠ¸ ì„ íƒìë“¤
        selectors = [
            'article[data-testid="post"]',
            'div[data-testid="post"]',
            'article[role="article"]',
            'div[role="article"]',
            'div[class*="post"]',
            'div[class*="thread"]',
            'article[class*="post"]',
            'article[class*="thread"]',
        ]
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    print(f"âœ… '{selector}' ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    
                    for element in elements:
                        post_data = self.extract_post_data(element)
                        if post_data:
                            posts.append(post_data)
                    
                    if posts:
                        break
            except Exception as e:
                continue
        
        return posts
    
    def extract_post_data(self, element):
        """í¬ìŠ¤íŠ¸ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì‚¬ìš©ìëª… ì¶”ì¶œ
            username = "ì•Œ ìˆ˜ ì—†ìŒ"
            username_selectors = [
                'a[href*="/@"]',
                '[data-testid="username"]',
                '[class*="username"]',
                'span[class*="username"]'
            ]
            
            for selector in username_selectors:
                try:
                    username_elem = element.select_one(selector)
                    if username_elem:
                        username = username_elem.get_text(strip=True)
                        break
                except:
                    continue
            
            # ë‚´ìš© ì¶”ì¶œ
            content = "ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            content_selectors = [
                'div[dir="auto"]',
                '[data-testid="post-text"]',
                '[class*="content"]',
                'p',
                'span[class*="text"]'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = element.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(strip=True)
                        if len(content) > 10:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
                            break
                except:
                    continue
            
            # ì‹œê°„ ì¶”ì¶œ
            timestamp = "ì‹œê°„ ì •ë³´ ì—†ìŒ"
            time_selectors = [
                'time',
                '[data-testid="timestamp"]',
                '[class*="time"]',
                'span[class*="time"]'
            ]
            
            for selector in time_selectors:
                try:
                    time_elem = element.select_one(selector)
                    if time_elem:
                        timestamp = time_elem.get('datetime') or time_elem.get_text(strip=True)
                        break
                except:
                    continue
            
            # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
            likes = "N/A"
            likes_selectors = [
                '[data-testid="like-count"]',
                '[class*="like"]',
                'span[class*="like"]'
            ]
            
            for selector in likes_selectors:
                try:
                    likes_elem = element.select_one(selector)
                    if likes_elem:
                        likes = likes_elem.get_text(strip=True)
                        break
                except:
                    continue
            
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            replies = "N/A"
            replies_selectors = [
                '[data-testid="reply-count"]',
                '[class*="reply"]',
                'span[class*="reply"]'
            ]
            
            for selector in replies_selectors:
                try:
                    replies_elem = element.select_one(selector)
                    if replies_elem:
                        replies = replies_elem.get_text(strip=True)
                        break
                except:
                    continue
            
            return {
                "username": username,
                "content": content[:200] + "..." if len(content) > 200 else content,
                "timestamp": timestamp,
                "likes": likes,
                "replies": replies
            }
            
        except Exception as e:
            print(f"âš ï¸ í¬ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def fallback_text_search(self, soup, keyword, max_posts):
        """ì¼ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ëŒ€ì²´ ë°©ë²•)"""
        results = []
        
        # í‚¤ì›Œë“œê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì°¾ê¸°
        keyword_matches = []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ë…¸ë“œì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        for text in soup.stripped_strings:
            if keyword.lower() in text.lower() and len(text.strip()) > 10:
                keyword_matches.append(text.strip())
        
        print(f"ğŸ“Š í‚¤ì›Œë“œ '{keyword}'ê°€ í¬í•¨ëœ {len(keyword_matches)}ê°œ í…ìŠ¤íŠ¸ ë°œê²¬")
        
        # ê²°ê³¼ ìƒì„±
        for i, content in enumerate(keyword_matches[:max_posts], 1):
            post_data = {
                "index": i,
                "username": f"ì‚¬ìš©ì_{i}",
                "content": content[:200] + "..." if len(content) > 200 else content,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "likes": "N/A",
                "replies": "N/A",
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "keyword": keyword,
                "method": "fallback_text_search"
            }
            results.append(post_data)
            self.print_post(post_data, i)
        
        return results
    
    def print_post(self, post_data, index):
        """í¬ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ í¬ìŠ¤íŠ¸ #{index}")
        print(f"ğŸ‘¤ ì‚¬ìš©ì: {post_data['username']}")
        print(f"â° ì‹œê°„: {post_data['timestamp']}")
        print(f"â¤ï¸ ì¢‹ì•„ìš”: {post_data['likes']}")
        print(f"ğŸ’¬ ëŒ“ê¸€: {post_data['replies']}")
        if 'keyword' in post_data:
            print(f"ğŸ” í‚¤ì›Œë“œ: {post_data['keyword']}")
        if 'method' in post_data:
            print(f"ğŸ”§ ë°©ë²•: {post_data['method']}")
        print(f"ğŸ“„ ë‚´ìš©:")
        print(f"   {post_data['content']}")
        print(f"ğŸ• í¬ë¡¤ë§ ì‹œê°„: {post_data['crawled_at']}")
        print(f"{'='*60}")
    
    def save_results(self, results, keyword):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if results:
            filename = f"final_threads_{keyword}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def demo_search(self):
        """ë°ëª¨ ê²€ìƒ‰ ì‹¤í–‰"""
        print("ğŸ¯ ë°ëª¨ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        
        # ìƒ˜í”Œ í‚¤ì›Œë“œë“¤
        sample_keywords = ["AI", "technology", "programming", "python"]
        
        for keyword in sample_keywords:
            print(f"\n{'='*60}")
            print(f"ğŸ” ë°ëª¨: '{keyword}' í‚¤ì›Œë“œ ê²€ìƒ‰")
            print(f"{'='*60}")
            
            results = self.search_threads(keyword, 3)
            
            if results:
                print(f"âœ… '{keyword}' ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
                self.save_results(results, keyword)
            else:
                print(f"âŒ '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            time.sleep(2)  # ìš”ì²­ ê°„ ë”œë ˆì´

def main():
    print("ğŸš€ Final Threads í¬ë¡¤ëŸ¬ ì‹œì‘!")
    print("=" * 60)
    print("ğŸ“ ì˜¬ë°”ë¥¸ URL êµ¬ì¡° ì‚¬ìš©: https://www.threads.com/search?q={keyword}&serp_type=default&hl=ko")
    print("=" * 60)
    
    crawler = FinalThreadsCrawler()
    
    try:
        while True:
            print("\nğŸ“‹ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
            print("1. í‚¤ì›Œë“œ ê²€ìƒ‰")
            print("2. ë°ëª¨ ì‹¤í–‰")
            print("3. ì¢…ë£Œ")
            
            choice = input("\nì„ íƒ (1-3): ").strip()
            
            if choice == "1":
                keyword = input("\nğŸ” ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if not keyword:
                    print("âŒ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                max_posts = input("ğŸ“Š ìµœëŒ€ ëª‡ ê°œì˜ í¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ê¸°ë³¸ê°’: 5): ").strip()
                max_posts = int(max_posts) if max_posts.isdigit() else 5
                
                print(f"\nğŸ¯ '{keyword}' í‚¤ì›Œë“œë¡œ ìµœëŒ€ {max_posts}ê°œ í¬ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
                
                results = crawler.search_threads(keyword, max_posts)
                
                if results:
                    print(f"\nâœ… ì´ {len(results)}ê°œì˜ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    
                    save_choice = input("\nğŸ’¾ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    if save_choice in ['y', 'yes', 'ì˜ˆ']:
                        crawler.save_results(results, keyword)
                else:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            elif choice == "2":
                crawler.demo_search()
            
            elif choice == "3":
                break
            
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-3 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            print("\n" + "="*60)
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        print("ğŸ‘‹ Final Threads í¬ë¡¤ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 