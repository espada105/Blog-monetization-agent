#!/usr/bin/env python3
"""
í•œêµ­ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
ë„¤ì´ë²„ ë‰´ìŠ¤ RSSì™€ ì—°í•©ë‰´ìŠ¤ RSSë¥¼ í™œìš©í•œ í•œêµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
"""

import asyncio
import aiohttp
import feedparser
from datetime import datetime, timedelta
import re
from typing import List, Dict, Optional
import logging

class KoreanNewsCrawler:
    def __init__(self):
        self.session = None
        self.setup_logging()
        
        # ì—°í•©ë‰´ìŠ¤ RSS URLë“¤ë§Œ ì‚¬ìš©
        self.yonhap_rss_urls = {
            'main': 'https://www.yonhapnews.co.kr/feed/headline/main.xml',
            'politics': 'https://www.yonhapnews.co.kr/feed/headline/politics.xml',
            'economy': 'https://www.yonhapnews.co.kr/feed/headline/economy.xml',
            'society': 'https://www.yonhapnews.co.kr/feed/headline/society.xml',
            'world': 'https://www.yonhapnews.co.kr/feed/headline/world.xml',
            'science': 'https://www.yonhapnews.co.kr/feed/headline/science.xml'
        }
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def fetch_rss_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with self.session.get(url, timeout=30) as response:
                if response.status == 200:
                    content = await response.text()
                    return feedparser.parse(content)
                else:
                    self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ìƒíƒœ: {response.status}")
                    return None
        except Exception as e:
            self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
            return None
    
    def parse_yonhap_news(self, feed: feedparser.FeedParserDict, category: str) -> List[Dict]:
        """ì—°í•©ë‰´ìŠ¤ íŒŒì‹±"""
        news_list = []
        
        for entry in feed.entries[:10]:  # ìµœì‹  10ê°œë§Œ
            try:
                # ì œëª© ì •ë¦¬
                title = entry.title.strip()
                
                # ë‚ ì§œ íŒŒì‹±
                pub_date = datetime.now()  # ê¸°ë³¸ê°’
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6])
                
                # ë§í¬
                link = entry.link if hasattr(entry, 'link') else ''
                
                # ìš”ì•½
                summary = ""
                if hasattr(entry, 'summary'):
                    summary = re.sub(r'<.*?>', '', entry.summary).strip()
                elif hasattr(entry, 'description'):
                    summary = re.sub(r'<.*?>', '', entry.description).strip()
                
                news_item = {
                    'title': title,
                    'link': link,
                    'summary': summary,
                    'category': category,
                    'source': 'ì—°í•©ë‰´ìŠ¤',
                    'published': pub_date.isoformat(),
                    'published_date': pub_date
                }
                
                news_list.append(news_item)
                
            except Exception as e:
                self.logger.error(f"ì—°í•©ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        return news_list
    
    async def get_category_news(self, category: str, limit: int = 5) -> List[Dict]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        all_news = []
        
        # ì—°í•©ë‰´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        if category in self.yonhap_rss_urls:
            feed = await self.fetch_rss_feed(self.yonhap_rss_urls[category])
            if feed:
                yonhap_news = self.parse_yonhap_news(feed, category)
                all_news.extend(yonhap_news[:limit//2])
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì¤‘ë³µ ì œê±°
        unique_news = self.remove_duplicates(all_news)
        return sorted(unique_news, key=lambda x: x['published_date'], reverse=True)[:limit]
    
    async def get_all_categories_today(self, limit_per_category: int = 3) -> List[Dict]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ì˜¤ëŠ˜ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        all_news = []
        
        # ì—°í•©ë‰´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        for category in ['main', 'politics', 'economy', 'society', 'world', 'science']:
            try:
                news = await self.get_category_news(category, limit_per_category)
                all_news.extend(news)
                self.logger.info(f"ì¹´í…Œê³ ë¦¬ '{category}'ì—ì„œ {len(news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            except Exception as e:
                self.logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_news = self.remove_duplicates(all_news)
        return sorted(unique_news, key=lambda x: x['published_date'], reverse=True)
    
    def remove_duplicates(self, news_list: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° (ì œëª© ê¸°ì¤€)"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            # ì œëª© ì •ê·œí™” (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)
            normalized_title = re.sub(r'\s+', ' ', news['title'].lower().strip())
            
            if normalized_title not in seen_titles:
                seen_titles.add(normalized_title)
                unique_news.append(news)
        
        return unique_news
    
    async def get_article_content(self, url: str) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            async with self.session.get(url, timeout=30) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # ê°„ë‹¨í•œ ë³¸ë¬¸ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
                    # ì—¬ê¸°ì„œëŠ” ìš”ì•½ë§Œ ë°˜í™˜
                    return "ë³¸ë¬¸ì€ ì›ë¬¸ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
                else:
                    return None
        except Exception as e:
            self.logger.error(f"ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
            return None

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    async with KoreanNewsCrawler() as crawler:
        print("ğŸ‡°ğŸ‡· í•œêµ­ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
        print("ğŸ“° IT ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        it_news = await crawler.get_category_news('it', 5)
        for news in it_news:
            print(f"- {news['title']} ({news['source']})")
        
        print("\n" + "=" * 50)
        
        # ì „ì²´ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
        print("ğŸ“° ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        all_news = await crawler.get_all_categories_today(3)
        print(f"ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        for news in all_news[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
            print(f"- [{news['category']}] {news['title']} ({news['source']})")

if __name__ == "__main__":
    asyncio.run(main()) 