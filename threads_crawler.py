import time
import json
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent
import re
from datetime import datetime

class ThreadsCrawler:
    def __init__(self):
        self.base_url = "https://www.threads.net"
        self.driver = None
        self.setup_driver()
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # User-Agent ì„¤ì •
        ua = UserAgent()
        chrome_options.add_argument(f"--user-agent={ua.random}")
        
        # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
    
    def search_threads(self, keyword, max_posts=10):
        """Threadsì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        try:
            # Threads ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
            search_url = f"{self.base_url}/search/{keyword}"
            print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¤‘...")
            print(f"ğŸ“± URL: {search_url}")
            
            self.driver.get(search_url)
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
            wait = WebDriverWait(self.driver, 10)
            
            # í¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì‹¤ì œ ì„ íƒìëŠ” Threads êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            posts = []
            
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒì ì‹œë„
            selectors = [
                "article",  # ì¼ë°˜ì ì¸ í¬ìŠ¤íŠ¸ ì„ íƒì
                "[data-testid='post']",  # í…ŒìŠ¤íŠ¸ ID ê¸°ë°˜
                ".post",  # í´ë˜ìŠ¤ ê¸°ë°˜
                "div[role='article']"  # ì—­í•  ê¸°ë°˜
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        print(f"âœ… '{selector}' ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                        posts = elements[:max_posts]
                        break
                except Exception as e:
                    continue
            
            if not posts:
                print("âŒ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return []
            
            results = []
            for i, post in enumerate(posts, 1):
                try:
                    post_data = self.extract_post_data(post, i)
                    if post_data:
                        results.append(post_data)
                        self.print_post(post_data, i)
                except Exception as e:
                    print(f"âš ï¸ í¬ìŠ¤íŠ¸ {i} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            return results
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def extract_post_data(self, post_element, index):
        """í¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì‚¬ìš©ìëª… ì¶”ì¶œ
            username = "ì•Œ ìˆ˜ ì—†ìŒ"
            try:
                username_elem = post_element.find_element(By.CSS_SELECTOR, "a[href*='/@']")
                username = username_elem.text.strip()
            except:
                pass
            
            # ë‚´ìš© ì¶”ì¶œ
            content = "ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            try:
                content_elem = post_element.find_element(By.CSS_SELECTOR, "div[dir='auto']")
                content = content_elem.text.strip()
            except:
                pass
            
            # ì‹œê°„ ì¶”ì¶œ
            timestamp = "ì‹œê°„ ì •ë³´ ì—†ìŒ"
            try:
                time_elem = post_element.find_element(By.CSS_SELECTOR, "time")
                timestamp = time_elem.get_attribute("datetime") or time_elem.text.strip()
            except:
                pass
            
            # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
            likes = "0"
            try:
                likes_elem = post_element.find_element(By.CSS_SELECTOR, "[data-testid='like-count']")
                likes = likes_elem.text.strip()
            except:
                pass
            
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            replies = "0"
            try:
                replies_elem = post_element.find_element(By.CSS_SELECTOR, "[data-testid='reply-count']")
                replies = replies_elem.text.strip()
            except:
                pass
            
            return {
                "index": index,
                "username": username,
                "content": content[:200] + "..." if len(content) > 200 else content,
                "timestamp": timestamp,
                "likes": likes,
                "replies": replies,
                "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def print_post(self, post_data, index):
        """í¬ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ í¬ìŠ¤íŠ¸ #{index}")
        print(f"ğŸ‘¤ ì‚¬ìš©ì: {post_data['username']}")
        print(f"â° ì‹œê°„: {post_data['timestamp']}")
        print(f"â¤ï¸ ì¢‹ì•„ìš”: {post_data['likes']}")
        print(f"ğŸ’¬ ëŒ“ê¸€: {post_data['replies']}")
        print(f"ğŸ“„ ë‚´ìš©:")
        print(f"   {post_data['content']}")
        print(f"ğŸ• í¬ë¡¤ë§ ì‹œê°„: {post_data['crawled_at']}")
        print(f"{'='*60}")
    
    def save_results(self, results, keyword):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if results:
            filename = f"threads_{keyword}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()

def main():
    print("ğŸš€ Threads í¬ë¡¤ëŸ¬ ì‹œì‘!")
    print("=" * 60)
    
    crawler = ThreadsCrawler()
    
    try:
        while True:
            keyword = input("\nğŸ” ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥): ").strip()
            
            if keyword.lower() == 'quit':
                break
            
            if not keyword:
                print("âŒ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            max_posts = input("ğŸ“Š ìµœëŒ€ ëª‡ ê°œì˜ í¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ê¹Œìš”? (ê¸°ë³¸ê°’: 5): ").strip()
            max_posts = int(max_posts) if max_posts.isdigit() else 5
            
            print(f"\nğŸ¯ '{keyword}' í‚¤ì›Œë“œë¡œ ìµœëŒ€ {max_posts}ê°œ í¬ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
            
            results = crawler.search_threads(keyword, max_posts)
            
            if results:
                print(f"\nâœ… ì´ {len(results)}ê°œì˜ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                
                # ê²°ê³¼ ì €ì¥ ì—¬ë¶€ í™•ì¸
                save_choice = input("\nğŸ’¾ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                if save_choice in ['y', 'yes', 'ì˜ˆ']:
                    crawler.save_results(results, keyword)
            else:
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print("\n" + "="*60)
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        crawler.close()
        print("ğŸ‘‹ Threads í¬ë¡¤ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 